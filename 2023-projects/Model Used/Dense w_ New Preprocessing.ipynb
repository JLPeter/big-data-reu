{"cells":[{"cell_type":"markdown","metadata":{"id":"LLBbMrjmpDOK"},"source":["# Preprocess only\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xqe9ZBBXKttI"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","import sys"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l7MXnaEwpGe5"},"outputs":[],"source":["#imports\n","print(\"Starting imports\")\n","#basics\n","import time\n","print(\"Starting timer.\")\n","startTime = time.time()\n","\n","import numpy as np\n","import pandas as pd\n","import datetime\n","import matplotlib.pyplot as plt\n","\n","#cleaning\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","\n","print(\"Imports Complete\")"]},{"cell_type":"markdown","metadata":{"id":"dmwL5liG52EI"},"source":["## Get Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q6tHhIxwpGnu"},"outputs":[],"source":["#paths\n","mainPath = '/content/drive/MyDrive/REU 2023 Team 1: Ice Bed Topography Prediction/Research/Lu_Folder/Data_derivedVelocity/Data_derivedVelocity/'\n","data_full_ = mainPath + 'data_full_vMag.csv'\n","data_1201_ = mainPath + 'd1201_vMag.csv'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CSd5zR4isB45"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ItKd5FYdum2N"},"outputs":[],"source":["#read data in\n","print(\"Reading Data In_\")\n","df_all = pd.read_csv(data_full_)\n","\n","df1201 = pd.read_csv(data_1201_)\n","print(\"Data read in completed.\")"]},{"cell_type":"markdown","metadata":{"id":"X2Ox7DBL55V_"},"source":["## Create functions from Homayra"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lWR-_GS1ZUKL"},"outputs":[],"source":["print(\"Establishing RMSPE functions.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G73ihBVVpGjN"},"outputs":[],"source":["def rmspe(y_true, y_pred):\n","    return np.sqrt(np.nanmean(np.square(((y_true - y_pred) / y_true))))*100"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HWNov_aZpGlX"},"outputs":[],"source":["def rmspe_1(y_true, y_pred):\n","    return np.sqrt(np.nanmean(np.square(y_true - y_pred) / y_true))*100"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cLAAj5qJZZp-"},"outputs":[],"source":["print(\"Function established.\")"]},{"cell_type":"markdown","metadata":{"id":"czPqgSZ25_Ps"},"source":["## Clean Data/Prepare Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iVF9u0eHx6Hf"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","\n","print(\"Begin data cleaning.\")\n","\n","df1201 = df1201.drop(columns=['Unnamed: 0.1'])\n","df_all = df_all.drop(columns=['Unnamed: 0.1'])\n","\n","# 1201 drop location variables\n","df1201_feats = df1201.drop(columns=['surf_x', 'surf_y'])\n","\n","# df_all drop location variables\n","df_all_feats_target = df_all.drop(columns=['surf_x', 'surf_y', 'track_bed_x', 'track_bed_y'])\n","\n","# 1201 order to align with df_all\n","df1201_feats_ordered = df1201_feats[['surf_vx', 'surf_vy', 'surf_elv', 'surf_dhdt', 'surf_SMB']]\n","\n","# set the feature variables to our independent characteristic variables\n","feature_cols = ['surf_vx', 'surf_vy', 'surf_elv', 'surf_dhdt', 'surf_SMB', 'v_mag']\n","\n","# split into X and Y\n","X_given = df_all_feats_target[feature_cols]\n","Y_given = df_all_feats_target['track_bed_target']\n","\n","#FIX\n","num_missing_cols = X_given.shape[1] - df1201_feats_ordered.shape[1]\n","if num_missing_cols > 0:\n","    missing_cols = np.zeros((df1201_feats_ordered.shape[0], num_missing_cols))\n","    df1201_feats_ordered = np.concatenate((df1201_feats_ordered, missing_cols), axis=1)\n","\n","# Combine all known X and validation 1201 X for standardizing\n","X_all = np.concatenate((X_given, df1201_feats_ordered))\n","\n","# make y into a dataframe to be standardized\n","Y_all = pd.DataFrame(Y_given)\n","\n","print(\"Data cleaned.\\nScaling beginning.\")\n","\n","# standardize\n","# Not setting feature range, let it be automatically determined\n","scaler_X = StandardScaler()\n","scaler_Y = StandardScaler()\n","\n","X_all_std = scaler_X.fit_transform(X_all)\n","Y_all_std = scaler_Y.fit_transform(Y_all)\n","\n","# can alternatively use the MinMaxScaler\n","print(\"Scaling Complete.\\nSplitting Data Beginning.\")\n","\n","# split of 1201 data from X_all_std\n","X_non1201 = X_all_std[0:632706, :]\n","X_1201_data = X_all_std[632706:, :]\n","\n","# # generate a randomseed for training and testing split\n","# generated = np.random.randint(0, 1000, 1)[0]\n","# print(f\"Generated random split for train-test: {generated}\")\n","generated = 168\n","\n","# set the train-test split\n","# 60-40 showed the most promising from previous research and additional testing\n","train_size_ = 0.6\n","# split training and test from df_all\n","x_train, x_test, y_train, y_test = train_test_split(\n","    X_non1201, Y_all_std, train_size=train_size_, test_size=1 - train_size_, random_state=generated\n",")\n","\n","# get validation data\n","val_split = 0.2  # can change as needed\n","x_train, x_val, y_train, y_val = train_test_split(\n","    x_train, y_train, train_size=1 - val_split, test_size=val_split, random_state=generated\n",")\n","\n","print(f\"Data train-split complete with: {train_size_ * 100}% training, {(1- train_size_) * 100}% testing, {val_split*100}% validation\")\n"]},{"cell_type":"markdown","metadata":{"id":"jmn7g_Dn6Fam"},"source":["## Modeling Here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PWni0zC6DClG"},"outputs":[],"source":["import keras,os\n","from keras.models import Sequential\n","from keras.layers import Dense, Flatten, Dropout, AveragePooling2D, LSTM, Activation, ConvLSTM2D, TimeDistributed, Input, Reshape\n","from keras.layers import UpSampling1D, Conv2DTranspose, UpSampling2D\n","from keras.optimizers import SGD\n","import numpy as np\n","\n","model = Sequential()\n","model.add(Dense(128, activation=\"relu\", input_shape=(x_train.shape[-1],)))\n","model.add(Dense(64, activation=\"relu\"))\n","model.add(Dropout(0.5))\n","model.add(Dense(32, activation=\"relu\"))\n","model.add(Dense(32, activation=\"relu\"))\n","model.add(Dropout(0.5))\n","model.add(Dense(16, activation=\"relu\"))\n","model.add(Dense(1, activation=\"linear\"))\n","model.compile(loss='mse', optimizer='adam')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"hXQhnHQaNrPR","outputId":"eda30b98-f9f0-4216-9d3b-10f9af3ca574"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense (Dense)               (None, 128)               896       \n","                                                                 \n"," dense_1 (Dense)             (None, 64)                8256      \n","                                                                 \n"," dropout (Dropout)           (None, 64)                0         \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                2080      \n","                                                                 \n"," dense_3 (Dense)             (None, 32)                1056      \n","                                                                 \n"," dropout_1 (Dropout)         (None, 32)                0         \n","                                                                 \n"," dense_4 (Dense)             (None, 16)                528       \n","                                                                 \n"," dense_5 (Dense)             (None, 1)                 17        \n","                                                                 \n","=================================================================\n","Total params: 12,833\n","Trainable params: 12,833\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/200\n","43/43 [==============================] - 3s 32ms/step - loss: 0.9301 - val_loss: 0.7299\n","Epoch 2/200\n","43/43 [==============================] - 1s 33ms/step - loss: 0.7001 - val_loss: 0.5639\n","Epoch 3/200\n","43/43 [==============================] - 2s 45ms/step - loss: 0.5833 - val_loss: 0.4925\n","Epoch 4/200\n","43/43 [==============================] - 2s 44ms/step - loss: 0.5321 - val_loss: 0.4634\n","Epoch 5/200\n","43/43 [==============================] - 1s 32ms/step - loss: 0.5015 - val_loss: 0.4400\n","Epoch 6/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.4791 - val_loss: 0.4362\n","Epoch 7/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.4627 - val_loss: 0.4079\n","Epoch 8/200\n","43/43 [==============================] - 1s 26ms/step - loss: 0.4475 - val_loss: 0.4128\n","Epoch 9/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.4325 - val_loss: 0.3902\n","Epoch 10/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.4206 - val_loss: 0.3833\n","Epoch 11/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.4099 - val_loss: 0.3726\n","Epoch 12/200\n","43/43 [==============================] - 1s 26ms/step - loss: 0.3997 - val_loss: 0.3606\n","Epoch 13/200\n","43/43 [==============================] - 1s 32ms/step - loss: 0.3878 - val_loss: 0.3623\n","Epoch 14/200\n","43/43 [==============================] - 2s 45ms/step - loss: 0.3806 - val_loss: 0.3475\n","Epoch 15/200\n","43/43 [==============================] - 2s 48ms/step - loss: 0.3746 - val_loss: 0.3444\n","Epoch 16/200\n","43/43 [==============================] - 1s 26ms/step - loss: 0.3671 - val_loss: 0.3366\n","Epoch 17/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.3600 - val_loss: 0.3375\n","Epoch 18/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.3549 - val_loss: 0.3361\n","Epoch 19/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.3511 - val_loss: 0.3223\n","Epoch 20/200\n","43/43 [==============================] - 2s 43ms/step - loss: 0.3465 - val_loss: 0.3314\n","Epoch 21/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.3415 - val_loss: 0.3355\n","Epoch 22/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.3403 - val_loss: 0.3295\n","Epoch 23/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.3335 - val_loss: 0.3187\n","Epoch 24/200\n","43/43 [==============================] - 2s 47ms/step - loss: 0.3314 - val_loss: 0.3243\n","Epoch 25/200\n","43/43 [==============================] - 2s 44ms/step - loss: 0.3300 - val_loss: 0.3281\n","Epoch 26/200\n","43/43 [==============================] - 2s 38ms/step - loss: 0.3273 - val_loss: 0.3189\n","Epoch 27/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.3230 - val_loss: 0.3252\n","Epoch 28/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.3221 - val_loss: 0.3130\n","Epoch 29/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.3186 - val_loss: 0.3205\n","Epoch 30/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.3155 - val_loss: 0.3207\n","Epoch 31/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.3137 - val_loss: 0.3146\n","Epoch 32/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.3118 - val_loss: 0.3373\n","Epoch 33/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.3123 - val_loss: 0.3129\n","Epoch 34/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.3096 - val_loss: 0.3202\n","Epoch 35/200\n","43/43 [==============================] - 2s 45ms/step - loss: 0.3050 - val_loss: 0.3174\n","Epoch 36/200\n","43/43 [==============================] - 2s 47ms/step - loss: 0.3039 - val_loss: 0.3245\n","Epoch 37/200\n","43/43 [==============================] - 2s 37ms/step - loss: 0.3012 - val_loss: 0.3223\n","Epoch 38/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2994 - val_loss: 0.3174\n","Epoch 39/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2979 - val_loss: 0.3129\n","Epoch 40/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2951 - val_loss: 0.3096\n","Epoch 41/200\n","43/43 [==============================] - 1s 29ms/step - loss: 0.2946 - val_loss: 0.3271\n","Epoch 42/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2929 - val_loss: 0.3089\n","Epoch 43/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2921 - val_loss: 0.3193\n","Epoch 44/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2909 - val_loss: 0.3131\n","Epoch 45/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2879 - val_loss: 0.3164\n","Epoch 46/200\n","43/43 [==============================] - 2s 43ms/step - loss: 0.2858 - val_loss: 0.3135\n","Epoch 47/200\n","43/43 [==============================] - 2s 47ms/step - loss: 0.2850 - val_loss: 0.3184\n","Epoch 48/200\n","43/43 [==============================] - 2s 38ms/step - loss: 0.2819 - val_loss: 0.3160\n","Epoch 49/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2837 - val_loss: 0.3205\n","Epoch 50/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2797 - val_loss: 0.3208\n","Epoch 51/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2778 - val_loss: 0.3300\n","Epoch 52/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2758 - val_loss: 0.3126\n","Epoch 53/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2771 - val_loss: 0.3341\n","Epoch 54/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2777 - val_loss: 0.3230\n","Epoch 55/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2725 - val_loss: 0.3144\n","Epoch 56/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2724 - val_loss: 0.3117\n","Epoch 57/200\n","43/43 [==============================] - 2s 44ms/step - loss: 0.2759 - val_loss: 0.3471\n","Epoch 58/200\n","43/43 [==============================] - 2s 43ms/step - loss: 0.2779 - val_loss: 0.3248\n","Epoch 59/200\n","43/43 [==============================] - 2s 42ms/step - loss: 0.2740 - val_loss: 0.3162\n","Epoch 60/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2713 - val_loss: 0.3099\n","Epoch 61/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2699 - val_loss: 0.3203\n","Epoch 62/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2709 - val_loss: 0.3159\n","Epoch 63/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2660 - val_loss: 0.3226\n","Epoch 64/200\n","43/43 [==============================] - 2s 44ms/step - loss: 0.2669 - val_loss: 0.3096\n","Epoch 65/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2654 - val_loss: 0.3145\n","Epoch 66/200\n","43/43 [==============================] - 1s 29ms/step - loss: 0.2627 - val_loss: 0.3267\n","Epoch 67/200\n","43/43 [==============================] - 2s 36ms/step - loss: 0.2638 - val_loss: 0.3149\n","Epoch 68/200\n","43/43 [==============================] - 2s 51ms/step - loss: 0.2659 - val_loss: 0.3220\n","Epoch 69/200\n","43/43 [==============================] - 2s 50ms/step - loss: 0.2632 - val_loss: 0.3203\n","Epoch 70/200\n","43/43 [==============================] - 2s 52ms/step - loss: 0.2589 - val_loss: 0.3251\n","Epoch 71/200\n","43/43 [==============================] - 2s 46ms/step - loss: 0.2630 - val_loss: 0.3263\n","Epoch 72/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2601 - val_loss: 0.3220\n","Epoch 73/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2586 - val_loss: 0.3176\n","Epoch 74/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2585 - val_loss: 0.3074\n","Epoch 75/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2614 - val_loss: 0.3174\n","Epoch 76/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2592 - val_loss: 0.3211\n","Epoch 77/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2545 - val_loss: 0.3221\n","Epoch 78/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2552 - val_loss: 0.3058\n","Epoch 79/200\n","43/43 [==============================] - 2s 43ms/step - loss: 0.2558 - val_loss: 0.3364\n","Epoch 80/200\n","43/43 [==============================] - 2s 47ms/step - loss: 0.2533 - val_loss: 0.3219\n","Epoch 81/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2509 - val_loss: 0.3170\n","Epoch 82/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2548 - val_loss: 0.3030\n","Epoch 83/200\n","43/43 [==============================] - 1s 29ms/step - loss: 0.2557 - val_loss: 0.3267\n","Epoch 84/200\n","43/43 [==============================] - 1s 29ms/step - loss: 0.2501 - val_loss: 0.3264\n","Epoch 85/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2512 - val_loss: 0.3265\n","Epoch 86/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2528 - val_loss: 0.3215\n","Epoch 87/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2511 - val_loss: 0.3201\n","Epoch 88/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2497 - val_loss: 0.3029\n","Epoch 89/200\n","43/43 [==============================] - 2s 42ms/step - loss: 0.2483 - val_loss: 0.3103\n","Epoch 90/200\n","43/43 [==============================] - 2s 44ms/step - loss: 0.2451 - val_loss: 0.3219\n","Epoch 91/200\n","43/43 [==============================] - 1s 31ms/step - loss: 0.2474 - val_loss: 0.3209\n","Epoch 92/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2455 - val_loss: 0.3084\n","Epoch 93/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2430 - val_loss: 0.3125\n","Epoch 94/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2474 - val_loss: 0.3325\n","Epoch 95/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2453 - val_loss: 0.3230\n","Epoch 96/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2411 - val_loss: 0.3182\n","Epoch 97/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2426 - val_loss: 0.3275\n","Epoch 98/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2431 - val_loss: 0.3226\n","Epoch 99/200\n","43/43 [==============================] - 1s 32ms/step - loss: 0.2433 - val_loss: 0.3213\n","Epoch 100/200\n","43/43 [==============================] - 2s 48ms/step - loss: 0.2455 - val_loss: 0.3251\n","Epoch 101/200\n","43/43 [==============================] - 2s 39ms/step - loss: 0.2452 - val_loss: 0.3181\n","Epoch 102/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2456 - val_loss: 0.3151\n","Epoch 103/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2440 - val_loss: 0.3133\n","Epoch 104/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2419 - val_loss: 0.3237\n","Epoch 105/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2395 - val_loss: 0.3264\n","Epoch 106/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2411 - val_loss: 0.3175\n","Epoch 107/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2422 - val_loss: 0.3177\n","Epoch 108/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2384 - val_loss: 0.3212\n","Epoch 109/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2383 - val_loss: 0.3121\n","Epoch 110/200\n","43/43 [==============================] - 2s 46ms/step - loss: 0.2379 - val_loss: 0.3171\n","Epoch 111/200\n","43/43 [==============================] - 2s 44ms/step - loss: 0.2371 - val_loss: 0.3153\n","Epoch 112/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2367 - val_loss: 0.3237\n","Epoch 113/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2369 - val_loss: 0.3194\n","Epoch 114/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2395 - val_loss: 0.3064\n","Epoch 115/200\n","43/43 [==============================] - 1s 29ms/step - loss: 0.2385 - val_loss: 0.3281\n","Epoch 116/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2387 - val_loss: 0.3255\n","Epoch 117/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2377 - val_loss: 0.3221\n","Epoch 118/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2370 - val_loss: 0.3265\n","Epoch 119/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2343 - val_loss: 0.3136\n","Epoch 120/200\n","43/43 [==============================] - 2s 41ms/step - loss: 0.2368 - val_loss: 0.3296\n","Epoch 121/200\n","43/43 [==============================] - 2s 48ms/step - loss: 0.2362 - val_loss: 0.3293\n","Epoch 122/200\n","43/43 [==============================] - 1s 31ms/step - loss: 0.2339 - val_loss: 0.3275\n","Epoch 123/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2387 - val_loss: 0.3291\n","Epoch 124/200\n","43/43 [==============================] - 1s 29ms/step - loss: 0.2350 - val_loss: 0.3322\n","Epoch 125/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2333 - val_loss: 0.3360\n","Epoch 126/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2330 - val_loss: 0.3311\n","Epoch 127/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2310 - val_loss: 0.3277\n","Epoch 128/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2340 - val_loss: 0.3291\n","Epoch 129/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2336 - val_loss: 0.3432\n","Epoch 130/200\n","43/43 [==============================] - 2s 38ms/step - loss: 0.2323 - val_loss: 0.3275\n","Epoch 131/200\n","43/43 [==============================] - 2s 46ms/step - loss: 0.2333 - val_loss: 0.3228\n","Epoch 132/200\n","43/43 [==============================] - 2s 36ms/step - loss: 0.2316 - val_loss: 0.3244\n","Epoch 133/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2304 - val_loss: 0.3252\n","Epoch 134/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2339 - val_loss: 0.3392\n","Epoch 135/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2347 - val_loss: 0.3520\n","Epoch 136/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2312 - val_loss: 0.3277\n","Epoch 137/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2304 - val_loss: 0.3434\n","Epoch 138/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2347 - val_loss: 0.3309\n","Epoch 139/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2319 - val_loss: 0.3388\n","Epoch 140/200\n","43/43 [==============================] - 1s 29ms/step - loss: 0.2302 - val_loss: 0.3236\n","Epoch 141/200\n","43/43 [==============================] - 2s 43ms/step - loss: 0.2290 - val_loss: 0.3438\n","Epoch 142/200\n","43/43 [==============================] - 2s 45ms/step - loss: 0.2309 - val_loss: 0.3248\n","Epoch 143/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2343 - val_loss: 0.3244\n","Epoch 144/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2306 - val_loss: 0.3345\n","Epoch 145/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2290 - val_loss: 0.3358\n","Epoch 146/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2273 - val_loss: 0.3234\n","Epoch 147/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2311 - val_loss: 0.3240\n","Epoch 148/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2243 - val_loss: 0.3326\n","Epoch 149/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2284 - val_loss: 0.3334\n","Epoch 150/200\n","43/43 [==============================] - 1s 29ms/step - loss: 0.2305 - val_loss: 0.3557\n","Epoch 151/200\n","43/43 [==============================] - 2s 38ms/step - loss: 0.2308 - val_loss: 0.3348\n","Epoch 152/200\n","43/43 [==============================] - 2s 48ms/step - loss: 0.2296 - val_loss: 0.3414\n","Epoch 153/200\n","43/43 [==============================] - 1s 33ms/step - loss: 0.2283 - val_loss: 0.3270\n","Epoch 154/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2278 - val_loss: 0.3367\n","Epoch 155/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2271 - val_loss: 0.3333\n","Epoch 156/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2259 - val_loss: 0.3282\n","Epoch 157/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2262 - val_loss: 0.3272\n","Epoch 158/200\n","43/43 [==============================] - 1s 29ms/step - loss: 0.2272 - val_loss: 0.3349\n","Epoch 159/200\n","43/43 [==============================] - 1s 29ms/step - loss: 0.2248 - val_loss: 0.3478\n","Epoch 160/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2247 - val_loss: 0.3251\n","Epoch 161/200\n","43/43 [==============================] - 2s 37ms/step - loss: 0.2245 - val_loss: 0.3322\n","Epoch 162/200\n","43/43 [==============================] - 2s 45ms/step - loss: 0.2231 - val_loss: 0.3365\n","Epoch 163/200\n","43/43 [==============================] - 2s 35ms/step - loss: 0.2243 - val_loss: 0.3449\n","Epoch 164/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2254 - val_loss: 0.3354\n","Epoch 165/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2253 - val_loss: 0.3496\n","Epoch 166/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2262 - val_loss: 0.3443\n","Epoch 167/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2230 - val_loss: 0.3527\n","Epoch 168/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2268 - val_loss: 0.3284\n","Epoch 169/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2269 - val_loss: 0.3271\n","Epoch 170/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2252 - val_loss: 0.3384\n","Epoch 171/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2242 - val_loss: 0.3483\n","Epoch 172/200\n","43/43 [==============================] - 2s 43ms/step - loss: 0.2232 - val_loss: 0.3403\n","Epoch 173/200\n","43/43 [==============================] - 2s 44ms/step - loss: 0.2252 - val_loss: 0.3473\n","Epoch 174/200\n","43/43 [==============================] - 1s 29ms/step - loss: 0.2237 - val_loss: 0.3373\n","Epoch 175/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2249 - val_loss: 0.3424\n","Epoch 176/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2271 - val_loss: 0.3471\n","Epoch 177/200\n","43/43 [==============================] - 1s 29ms/step - loss: 0.2220 - val_loss: 0.3556\n","Epoch 178/200\n","43/43 [==============================] - 1s 29ms/step - loss: 0.2203 - val_loss: 0.3464\n","Epoch 179/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2245 - val_loss: 0.3502\n","Epoch 180/200\n","43/43 [==============================] - 1s 29ms/step - loss: 0.2220 - val_loss: 0.3556\n","Epoch 181/200\n","43/43 [==============================] - 2s 37ms/step - loss: 0.2296 - val_loss: 0.3536\n","Epoch 182/200\n","43/43 [==============================] - 2s 59ms/step - loss: 0.2232 - val_loss: 0.3427\n","Epoch 183/200\n","43/43 [==============================] - 2s 57ms/step - loss: 0.2223 - val_loss: 0.3468\n","Epoch 184/200\n","43/43 [==============================] - 2s 53ms/step - loss: 0.2207 - val_loss: 0.3546\n","Epoch 185/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2214 - val_loss: 0.3597\n","Epoch 186/200\n","43/43 [==============================] - 1s 29ms/step - loss: 0.2209 - val_loss: 0.3534\n","Epoch 187/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2205 - val_loss: 0.3507\n","Epoch 188/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2214 - val_loss: 0.3550\n","Epoch 189/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2195 - val_loss: 0.3485\n","Epoch 190/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2206 - val_loss: 0.3368\n","Epoch 191/200\n","43/43 [==============================] - 1s 29ms/step - loss: 0.2200 - val_loss: 0.3468\n","Epoch 192/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2193 - val_loss: 0.3440\n","Epoch 193/200\n","43/43 [==============================] - 2s 44ms/step - loss: 0.2204 - val_loss: 0.3358\n","Epoch 194/200\n","43/43 [==============================] - 2s 44ms/step - loss: 0.2204 - val_loss: 0.3514\n","Epoch 195/200\n","43/43 [==============================] - 1s 31ms/step - loss: 0.2188 - val_loss: 0.3564\n","Epoch 196/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2194 - val_loss: 0.3488\n","Epoch 197/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2222 - val_loss: 0.3592\n","Epoch 198/200\n","43/43 [==============================] - 1s 28ms/step - loss: 0.2193 - val_loss: 0.3565\n","Epoch 199/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2180 - val_loss: 0.3448\n","Epoch 200/200\n","43/43 [==============================] - 1s 27ms/step - loss: 0.2209 - val_loss: 0.3361\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7f9db4ffb0a0>"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["model.summary()\n","model.compile(optimizer='adam', loss='mse')\n","model.fit(x_train, y=y_train, epochs=200, batch_size=5000, verbose=1, validation_split =0.3, shuffle=True\n","          )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kR-WyzBLNt0Z"},"outputs":[],"source":["y_pred_train = model.predict(x_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"U7Z-9hHzN0CT","outputId":"057ad1ab-9fd9-4a3a-ecd7-7d8d90bca871"},"outputs":[{"data":{"text/plain":["((303698, 1), (303698, 1))"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["y_pred_train.shape, y_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r1RTqk-4N3T1"},"outputs":[],"source":["train_Predict_full_range = scaler_Y.inverse_transform(y_pred_train)\n","data_train_Y_full_range = scaler_Y.inverse_transform(y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q-SGzynkpjNf"},"outputs":[],"source":["from sklearn.metrics import mean_absolute_error, r2_score, mean_absolute_percentage_error\n","\n","print('RMSE:',np.sqrt(mean_squared_error(data_train_Y_full_range, train_Predict_full_range)))\n","print('RMSE Percentage:',rmspe(data_train_Y_full_range, train_Predict_full_range))\n","print('RMSE Percentage-1:',rmspe_1(data_train_Y_full_range, train_Predict_full_range))\n","print('Mean Absolute Error:', mean_absolute_error(data_train_Y_full_range, train_Predict_full_range))\n","print('Mean Absolute Percentage Error:', mean_absolute_percentage_error(data_train_Y_full_range, train_Predict_full_range))\n","print('R^2 Score:', r2_score(data_train_Y_full_range, train_Predict_full_range))"]},{"cell_type":"markdown","metadata":{"id":"Tg7UL0HUOF2p"},"source":["Prediction of track bed on test data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"22M1NtGqOEel"},"outputs":[],"source":["y_predict_test = model.predict(x_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FnkhTbAdOHyW"},"outputs":[],"source":["y_test.shape, y_predict_test.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5m1pBDzKOMw_"},"outputs":[],"source":["test_Predict_full_range = scaler_Y.inverse_transform(y_predict_test)\n","data_test_Y_full_range = scaler_Y.inverse_transform(y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M277OkbsOPaM"},"outputs":[],"source":["from sklearn.metrics import mean_absolute_error, r2_score, mean_absolute_percentage_error\n","\n","print('RMSE:',np.sqrt(mean_squared_error(data_test_Y_full_range, test_Predict_full_range)))\n","print('RMSE Percentage:',rmspe(data_test_Y_full_range, test_Predict_full_range))\n","print('RMSE Percentage-1:',rmspe_1(data_test_Y_full_range, test_Predict_full_range))\n","print('Mean Absolute Error:', mean_absolute_error(data_test_Y_full_range, test_Predict_full_range))\n","print('Mean Absolute Percentage Error:', mean_absolute_percentage_error(data_test_Y_full_range, test_Predict_full_range))\n","print('R^2 Score:', r2_score(data_test_Y_full_range, test_Predict_full_range))"]},{"cell_type":"markdown","metadata":{"id":"On3zV9flTtr8"},"source":["## Metrics and Viz Here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0rfgGT9CUB1-"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.subplots(figsize=(15, 5))\n","\n","plt.plot(data_train_Y_full_range, color='red', label=\"Ground Truth\") # Y_test_given,y_pred_test\n","plt.plot(train_Predict_full_range, color='blue', label=\"Prediction\")\n","plt.ylabel(\"Ice Bed Height\")\n","plt.xlabel(\"Data Elements\")\n","plt.legend(loc=\"upper left\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sIpIyWfDaqNQ"},"outputs":[],"source":["endTime = time.time()\n","print(f\"Total Time Taken: {endTime - startTime:.03f}ms\")\n","print(\"Modeling COMPLETE\")"]}],"metadata":{"colab":{"provenance":[{"file_id":"1pjG0L4NKwKkc5TUZHXv3QLQzffrJWRqP","timestamp":1689955688432},{"file_id":"1lVz9i8_U4DPkd8Ji_pr0I8CU10Z9X5u7","timestamp":1689609386417},{"file_id":"1Qhmbg_5KXoPeX8CkPK_UF80403LdyQ2h","timestamp":1687381723841}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}