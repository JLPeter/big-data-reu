run_id: 'jh7.31_lstm23_b'
pred_ckpt: '/nfs/rs/cybertrn/reu2024/team2/base/logs/tb_logs/jh7.31_lstm23_b/lightning_logs/version_0/checkpoints/epoch=2499-step=952500.ckpt'  # only use if doing inference
resume_ckpt: ''
mdl_key: 'LSTM23'

data:  # shouldn't be deleting any of these
  train_data_path: '/nfs/rs/cybertrn/reu2024/team2/base/pp2/data/barajas/train/'  # must include '/' at end
  test_data_path: '/nfs/rs/cybertrn/reu2024/team2/base/pp2/data/barajas/test/'
  batch_size: 2048
  val_split: 0.1

fit:  # shouldn't be deleting any of these
  max_epochs: 2500
  n_nodes: 1
  n_devices: 2
  patience: 2500
  ckpt_freq: 300

model:
  indim: 15
  outdim: 13
  num_layers: 4
  neurons: 128
  lr: 0.001
  lr_step: 450
  lr_gam: 0.95
  l2: 0.0000001
  dropout: 0.45
  activation: 'relu'
  optimizer: 'adam'

