# shouldn't be deleting any of these
run_id: 'jh8.1_lstm23_shobe_v2'
pred_ckpt: '/nfs/rs/cybertrn/reu2024/team2/base/logs/ckpts/ep7.30_l2_shobe1/epoch=499-step=175500.ckpt'  # only change if doing inference
resume_ckpt: ''  # leave as '' if not resuming
mdl_key: 'LSTM23'

data:  # shouldn't be deleting any of these
  train_data_path: '/nfs/rs/cybertrn/reu2024/team2/base/pp2/data/shakeri-obe/train/'  # must include '/' at end
  test_data_path: '/nfs/rs/cybertrn/reu2024/team2/base/pp2/data/shakeri-obe/test/'
  batch_size: 1024
  val_split: .2


fit:  # shouldn't be deleting any of these
  max_epochs: 3000 
  n_nodes: 2
  n_devices: 4
  patience: 3000 # make arbitrarily large to turn off early stopping
  ckpt_freq: 500  # dumb checkpointing: saves every n epochs

model:  # should match the model you are using
  indim: 15
  outdim: 13
  num_layers: 16
  neurons: 256
  lr: 0.01
  lr_step: 400
  lr_gam: 0.95
  l2: .01
  dropout: 0.45
  activation: 'leakyrelu'
  optimizer: 'adamw'



  