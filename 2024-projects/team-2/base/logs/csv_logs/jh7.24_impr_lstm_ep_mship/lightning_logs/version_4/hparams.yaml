activation: relu
dropout: 0.15
indim: 15
lr: 0.001
lr_gam: 0.95
lr_step: 400
neurons: 256
num_layers: 16
optimizer: adam
outdim: 13
penalty: 0.0001
