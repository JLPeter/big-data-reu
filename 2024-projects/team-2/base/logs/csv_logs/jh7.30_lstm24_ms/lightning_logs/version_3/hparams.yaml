activation: relu
bias: false
custom_loss: true
dropout: 0.3
hidden_state_size: 256
indim: 15
input_neurons: 256
l2: 0.0
lr: 0.001
lr_gam: 0.95
lr_step: 400
neurons_per_hidden:
- 2048
- 1024
- 678
- 576
- 512
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 64
- 32
- 16
num_linears: 16
num_lstm_layers: 4
one_activation: false
optimizer: adam
outdim: 13
penalty: 1
