activation: relu
dropout: 0.5
hidden_layers:
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 256
input_size: 15
lr: 0.001
lr_gam: 0.95
lr_step: 400
num_classes: 13
penalty: 1.0e-05
