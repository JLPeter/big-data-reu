activation: relu
dropout: 0.1
hidden_layers:
- 512
- 256
- 256
- 256
- 256
- 128
- 128
- 128
- 128
- 64
- 32
- 16
input_size: 15
l2: 0.01
lr: 0.0008
lr_gam: 0.95
lr_step: 250
num_classes: 13
penalty: 1.01
