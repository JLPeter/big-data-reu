activation: relu
dropout: 0.0
indim: 15
l2: 0
log_test: false
lr: 0.001
lr_gam: 0.1
lr_step: 2000
neurons: 128
num_layers: 4
optimizer: adam
outdim: 13
