activation: relu
bias: false
custom_loss: false
dropout: 0
hidden_state_size: 128
indim: 15
input_neurons: 128
l2: 0
lr: 0.001
lr_gam: 0.1
lr_step: 2000
neurons_per_hidden:
- 128
- 64
num_linears: 2
num_lstm_layers: 4
one_activation: false
optimizer: adamw
outdim: 13
penalty: 0
