activation: relu
dropout: 0.2
indim: 15
l2: 0.01
log_test: false
lr: 0.001
lr_gam: 0.95
lr_step: 200
neurons: 256
num_layers: 12
optimizer: adamw
outdim: 13
