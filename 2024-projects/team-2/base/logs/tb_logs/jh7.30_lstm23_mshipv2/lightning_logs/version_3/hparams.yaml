activation: relu
dropout: 0.4
indim: 15
l2: 1.0e-07
log_test: false
lr: 0.001
lr_gam: 0.95
lr_step: 100
neurons: 128
num_layers: 2
optimizer: adam
outdim: 13
