activation: relu
dropout: 0.2
hidden_layers:
- 512
- 448
- 394
- 192
- 128
- 64
- 32
- 16
input_size: 15
lr: 0.001
lr_gam: 0.95
lr_step: 400
num_classes: 13
penalty: 0.0001
